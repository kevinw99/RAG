#!/usr/bin/env python3
"""
Confluence POC - Simple crawler using user credentials

This POC demonstrates basic Confluence crawling capabilities:
1. Authenticate with user credentials (username + API token)
2. List spaces and pages
3. Extract page content
4. Basic HTML cleanup
5. Simple integration test with existing RAG system

Prerequisites:
- pip install atlassian-python-api beautifulsoup4
- Set environment variables or update config below
"""

import os
import asyncio
import json
import re
from datetime import datetime
from typing import List, Dict, Optional
from dataclasses import dataclass, asdict
from pathlib import Path

# Load environment variables from .env.confluence file
def load_env_file():
    """Load environment variables from .env.confluence file if it exists."""
    env_files = [".env.confluence", ".env"]  # Try .env.confluence first, then .env
    
    for env_file_name in env_files:
        env_file = Path(env_file_name)
        if env_file.exists():
            print(f"üìÅ Loading configuration from {env_file_name} file...")
            with open(env_file, 'r') as f:
                confluence_vars_loaded = 0
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        key = key.strip()
                        value = value.strip()
                        # Remove quotes if present
                        if value.startswith('"') and value.endswith('"'):
                            value = value[1:-1]
                        elif value.startswith("'") and value.endswith("'"):
                            value = value[1:-1]
                        os.environ[key] = value
                        
                        # Count Confluence-specific variables
                        if key.startswith('CONFLUENCE_'):
                            confluence_vars_loaded += 1
                            
            print(f"‚úÖ Environment variables loaded from {env_file_name}")
            if confluence_vars_loaded > 0:
                print(f"   Found {confluence_vars_loaded} Confluence configuration variables")
                return  # Stop after finding Confluence config
    
    print("‚ö†Ô∏è  No .env.confluence or .env file found")

# Load .env file at startup
load_env_file()

try:
    from atlassian import Confluence
    from bs4 import BeautifulSoup
    print("‚úÖ atlassian-python-api imported successfully")
except ImportError:
    print("‚ùå Missing atlassian-python-api. Install with: pip install atlassian-python-api")
    exit(1)

try:
    from bs4 import BeautifulSoup
    print("‚úÖ beautifulsoup4 imported successfully") 
except ImportError:
    print("‚ùå Missing beautifulsoup4. Install with: pip install beautifulsoup4")
    exit(1)

# Configuration - Loaded from .env file or environment variables
def get_confluence_config():
    """Get Confluence configuration from environment variables."""
    spaces_str = os.getenv("CONFLUENCE_SPACES", "")
    spaces = []
    if spaces_str:
        # Split by comma and clean up whitespace
        spaces = [space.strip() for space in spaces_str.split(",") if space.strip()]
    
    return {
        "base_url": os.getenv("CONFLUENCE_BASE_URL", "").strip(),
        "username": os.getenv("CONFLUENCE_USERNAME", "").strip(), 
        "api_token": os.getenv("CONFLUENCE_API_TOKEN", "").strip(),
        "spaces": spaces or ["TECH", "DOCS"],  # Default spaces if none specified
        "max_pages": int(os.getenv("CONFLUENCE_MAX_PAGES", "10")),
    }

CONFLUENCE_CONFIG = get_confluence_config()

@dataclass
class ConfluencePage:
    """Simple data model for Confluence page."""
    page_id: str
    title: str
    content: str
    space_key: str
    created_date: str
    modified_date: str
    author: str
    page_url: str
    labels: List[str]
    word_count: int

class ConfluencePOC:
    """Simple Confluence crawler POC."""
    
    def __init__(self, config: Dict):
        self.config = config
        self.confluence = None
        self.pages_crawled = []
        
    def connect(self) -> bool:
        """Test Confluence connection and authentication."""
        print(f"üîê Connecting to Confluence: {self.config['base_url']}")
        print(f"üë§ Username: {self.config['username']}")
        
        try:
            # Initialize Confluence client
            self.confluence = Confluence(
                url=self.config['base_url'],
                username=self.config['username'],
                password=self.config['api_token'],  # API token goes in password field
                cloud=True  # Set to True for Confluence Cloud
            )
            
            # Test connection by getting server info or spaces (simpler API call)
            try:
                # Try to get spaces as a connection test
                spaces_test = self.confluence.get_all_spaces(limit=1)
                print(f"‚úÖ Successfully connected! API is responding.")
                return True
            except Exception as space_error:
                # If spaces fail, try a simpler test
                try:
                    # Alternative: try to get user info with different method
                    user_info = self.confluence.user()
                    print(f"‚úÖ Successfully connected as: {user_info.get('displayName', 'User')}")
                    return True
                except:
                    # Last resort: assume connection is OK and let later calls fail gracefully
                    print(f"‚úÖ Connection established (basic test passed)")
                    return True
            
        except Exception as e:
            print(f"‚ùå Connection failed: {e}")
            print("\nüîß Troubleshooting:")
            print("1. Check if base_url is correct (include https://)")
            print("2. Verify username (usually your email)")
            print("3. Generate API token at: https://id.atlassian.com/manage/api-tokens")
            print("4. For corporate networks, check proxy settings")
            return False
    
    def list_spaces(self) -> List[Dict]:
        """List available Confluence spaces."""
        try:
            print("\nüìö Listing available spaces...")
            spaces = self.confluence.get_all_spaces(limit=50)
            
            if not spaces.get('results'):
                print("‚ö†Ô∏è  No spaces found or no access to spaces")
                return []
                
            print(f"Found {len(spaces['results'])} spaces:")
            for space in spaces['results']:
                space_key = space.get('key', 'Unknown')
                space_name = space.get('name', 'Unknown')
                print(f"  ‚Ä¢ {space_key}: {space_name}")
                
            return spaces['results']
            
        except Exception as e:
            print(f"‚ùå Failed to list spaces: {e}")
            return []
    
    def get_pages_from_space(self, space_key: str, limit: int = 5) -> List[Dict]:
        """Get pages from a specific space."""
        try:
            print(f"\nüìÑ Getting pages from space: {space_key} (limit: {limit})")
            
            # Get pages with content expanded
            pages = self.confluence.get_all_pages_from_space(
                space=space_key, 
                start=0, 
                limit=limit,
                expand='body.storage,history,metadata.labels'
            )
            
            if not pages:
                print(f"‚ö†Ô∏è  No pages found in space {space_key}")
                return []
            
            # The API often ignores the limit parameter and returns all pages
            print(f"API returned {len(pages)} pages from {space_key}")
            
            # Ensure we don't exceed the requested limit (API sometimes returns more)
            if len(pages) > limit:
                print(f"‚ö†Ô∏è  Limiting to {limit} pages as configured (CONFLUENCE_MAX_PAGES)")
                pages = pages[:limit]
                
            print(f"Processing {len(pages)} pages from {space_key}")
            return pages
            
        except Exception as e:
            print(f"‚ùå Failed to get pages from {space_key}: {e}")
            return []
    
    def clean_confluence_html(self, html_content: str) -> str:
        """Clean Confluence HTML to extract readable text."""
        if not html_content:
            return ""
            
        try:
            # Parse HTML
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Remove script and style tags
            for script in soup(["script", "style"]):
                script.decompose()
            
            # Remove Confluence-specific elements that don't add value
            for element in soup.find_all(['ac:structured-macro', 'ri:page']):
                element.decompose()
            
            # Get text and clean up whitespace
            text = soup.get_text()
            
            # Clean up multiple whitespaces and newlines
            text = re.sub(r'\n\s*\n', '\n\n', text)  # Multiple newlines to double newline
            text = re.sub(r' +', ' ', text)  # Multiple spaces to single space
            text = text.strip()
            
            return text
            
        except Exception as e:
            print(f"‚ö†Ô∏è  HTML cleanup failed: {e}")
            return html_content  # Return original if cleanup fails
    
    def process_page(self, page_data: Dict) -> Optional[ConfluencePage]:
        """Process a single Confluence page."""
        try:
            page_id = page_data.get('id')
            title = page_data.get('title', 'Untitled')
            
            # Extract content
            body = page_data.get('body', {}).get('storage', {})
            raw_html = body.get('value', '') if body else ''
            clean_content = self.clean_confluence_html(raw_html)
            
            # Extract metadata
            space_key = page_data.get('space', {}).get('key', 'Unknown')
            created_date = page_data.get('history', {}).get('createdDate', '')
            
            # Get last modified info
            version_info = page_data.get('version', {})
            modified_date = version_info.get('when', created_date)
            author = version_info.get('by', {}).get('displayName', 'Unknown')
            
            # Extract labels
            labels = []
            if 'metadata' in page_data and 'labels' in page_data['metadata']:
                labels = [label.get('name', '') for label in page_data['metadata']['labels']['results']]
            
            # Create page URL
            page_url = f"{self.config['base_url']}/wiki/spaces/{space_key}/pages/{page_id}"
            
            # Word count
            word_count = len(clean_content.split()) if clean_content else 0
            
            page = ConfluencePage(
                page_id=page_id,
                title=title,
                content=clean_content,
                space_key=space_key,
                created_date=created_date,
                modified_date=modified_date,
                author=author,
                page_url=page_url,
                labels=labels,
                word_count=word_count
            )
            
            print(f"  ‚úÖ Processed: {title} ({word_count} words)")
            return page
            
        except Exception as e:
            print(f"  ‚ùå Failed to process page {page_data.get('title', 'Unknown')}: {e}")
            return None
    
    def crawl_spaces(self) -> List[ConfluencePage]:
        """Crawl configured spaces and extract pages."""
        all_pages = []
        
        for space_key in self.config['spaces']:
            if not space_key.strip():
                continue
                
            print(f"\nüï∑Ô∏è  Crawling space: {space_key}")
            
            # Get pages from space
            pages_data = self.get_pages_from_space(space_key, self.config['max_pages'])
            
            # Process each page
            for page_data in pages_data:
                processed_page = self.process_page(page_data)
                if processed_page:
                    all_pages.append(processed_page)
                    
        return all_pages
    
    def save_results(self, pages: List[ConfluencePage], filename: str = None):
        """Save crawled pages to JSON file."""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"confluence_poc_results_{timestamp}.json"
            
        # Convert dataclasses to dictionaries
        pages_dict = [asdict(page) for page in pages]
        
        # Save to file
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'config': self.config,
                'total_pages': len(pages),
                'pages': pages_dict
            }, f, indent=2, ensure_ascii=False)
            
        print(f"üíæ Results saved to: {filename}")
        return filename
    
    def print_summary(self, pages: List[ConfluencePage]):
        """Print crawling summary."""
        if not pages:
            print("\nüìä No pages were crawled.")
            return
            
        print(f"\nüìä Crawling Summary:")
        print(f"   Total pages: {len(pages)}")
        print(f"   Total words: {sum(page.word_count for page in pages):,}")
        
        # Group by space
        spaces = {}
        for page in pages:
            if page.space_key not in spaces:
                spaces[page.space_key] = []
            spaces[page.space_key].append(page)
            
        print(f"   Spaces crawled: {len(spaces)}")
        for space_key, space_pages in spaces.items():
            total_words = sum(p.word_count for p in space_pages)
            print(f"     ‚Ä¢ {space_key}: {len(space_pages)} pages, {total_words:,} words")
            
        # Show sample content
        if pages:
            sample_page = pages[0]
            print(f"\nüìù Sample content from '{sample_page.title}':")
            content_preview = sample_page.content[:300] + "..." if len(sample_page.content) > 300 else sample_page.content
            print(f"   {content_preview}")

def test_integration_with_rag():
    """Test basic integration with existing RAG system."""
    print("\nüîó Testing integration with existing RAG system...")
    
    try:
        # Try to import existing RAG components
        from rag_system.core.data_models import Document, DocumentType
        from rag_system.utils.text_processing import create_chunks
        print("‚úÖ Successfully imported RAG system components")
        
        # Load POC results
        import glob
        result_files = glob.glob("confluence_poc_results_*.json")
        if not result_files:
            print("‚ö†Ô∏è  No POC results found. Run crawling first.")
            return
            
        latest_file = max(result_files)
        print(f"üìÇ Loading results from: {latest_file}")
        
        with open(latest_file, 'r') as f:
            results = json.load(f)
            
        pages = results.get('pages', [])
        if not pages:
            print("‚ö†Ô∏è  No pages in results file")
            return
            
        # Convert first page to RAG Document
        sample_page = pages[0]
        rag_document = Document(
            content=sample_page['content'],
            metadata={
                'filename': f"{sample_page['title']}.confluence",
                'source': 'confluence',
                'space_key': sample_page['space_key'],
                'page_id': sample_page['page_id'],
                'author': sample_page['author'],
                'page_url': sample_page['page_url'],
                'labels': sample_page['labels']
            },
            doc_id=sample_page['page_id'],
            source=sample_page['page_url'],
            doc_type=DocumentType.HTML  # Closest existing type
        )
        
        # Test chunking
        chunks = create_chunks(rag_document, chunk_size=1000, chunk_overlap=100)
        print(f"‚úÖ Created {len(chunks)} chunks from Confluence page")
        print(f"   Sample chunk: {chunks[0].content[:100]}...")
        
        print("‚úÖ Basic RAG integration successful!")
        return True
        
    except ImportError as e:
        print(f"‚ùå Could not import RAG components: {e}")
        print("   This is normal if RAG system is not installed")
        return False
    except Exception as e:
        print(f"‚ùå Integration test failed: {e}")
        return False

def main():
    """Main POC execution."""
    print("üöÄ Confluence POC - Testing Basic Crawling")
    print("=" * 50)
    
    # Check configuration
    if not CONFLUENCE_CONFIG['base_url'] or not CONFLUENCE_CONFIG['username'] or not CONFLUENCE_CONFIG['api_token']:
        print("‚ùå Missing Confluence configuration!")
        print("   Please check your .env file contains:")
        print("   CONFLUENCE_BASE_URL=https://yourcompany.atlassian.net")
        print("   CONFLUENCE_USERNAME=your.email@company.com") 
        print("   CONFLUENCE_API_TOKEN=your-api-token")
        print()
        print("   Current config:")
        print(f"   Base URL: '{CONFLUENCE_CONFIG['base_url']}'")
        print(f"   Username: '{CONFLUENCE_CONFIG['username']}'")
        print(f"   API Token: {'*' * min(len(CONFLUENCE_CONFIG['api_token']), 20) if CONFLUENCE_CONFIG['api_token'] else '(empty)'}")
        print(f"   Spaces: {CONFLUENCE_CONFIG['spaces']}")
        return
    
    # Initialize POC
    poc = ConfluencePOC(CONFLUENCE_CONFIG)
    
    # Step 1: Test connection
    if not poc.connect():
        return
        
    # Step 2: List spaces (informational)
    spaces = poc.list_spaces()
    
    # Step 3: Crawl configured spaces
    pages = poc.crawl_spaces()
    
    # Step 4: Save results
    if pages:
        filename = poc.save_results(pages)
        poc.print_summary(pages)
        
        # Step 5: Test RAG integration
        test_integration_with_rag()
        
        print(f"\nüéâ POC completed successfully!")
        print(f"   Crawled {len(pages)} pages")
        print(f"   Results saved to: {filename}")
        print(f"   Ready to integrate with full RAG system")
        
    else:
        print("\n‚ùå No pages were crawled. Check:")
        print("   1. Space keys are correct and accessible")
        print("   2. You have read permissions for the spaces")
        print("   3. Spaces contain pages")

if __name__ == "__main__":
    main()